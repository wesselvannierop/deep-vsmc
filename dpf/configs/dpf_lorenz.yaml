# General
n_particles: 28
likelihood_sigma: 0.1

data:
  salt_prob: 0.0
  awgn_std: [0.0, 0.6]
  image_shape: [28, 28]
  # normalization_range: [-1, 1]
  observation_fn: mask
  observation_fn_kwargs:
    block_size: 4
    p: [0.0, 0.9]
sample_dt: 0.02

prior: null
clip_range: [-50.0, 50.0] # clip the particles to this range (x and y could be [-35, 35])
initial_state: [0.0, 35.0] # mu, sigma
transition:
  learn: True
  sigma: 7.0 # only used if learn is False

model: learned # bootstrap, learned
experiment: lorenz

sequence_length: 8
batch_size: 48
# batch_size: 64 # snellius
nr_of_sequences: 1024
# one epoch = 32 steps

val_sequence_length: 128
val_batch_size: 32
val_nr_of_sequences: 32 # == val_batch_size
wandb:
  project: dpf
  entity: wessel

save_path: "{output_dir}/dpf/{experiment}/{timestamp}-{model}-training={training.train}-p_occlusion={data.observation_fn_kwargs.p}-awgn={data.awgn_std[1]}"

# For training
training:
  train: True  # if True, train the model, otherwise load the trained model
  n_retrain: 1
  epochs: 120 # this is 32 * 90 = 2880 iterations
  all_observations_at_epoch: 30
  optimizer:
    learning_rate: 1.e-4
    global_clipnorm: 1000.0
  mixture: 2
  equally_weighted_mixture: False
  validation_freq: 3
  enhance_gradient_steps: 2 # maybe try 2 again

validation:
  visualize_idx: 0
  enable_compute_elbo: True
  dump_results: True